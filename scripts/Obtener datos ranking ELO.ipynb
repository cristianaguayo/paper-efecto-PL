{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener datos ranking ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.worldfootball.net\n",
    "# Librerías de siempre\n",
    "import os\n",
    "os.system(\"taskset -p 0xff %d\" % os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from time import time\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import re\n",
    "import fnmatch\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_elo_test = pd.read_excel(os.path.join(os.path.pardir, 'Datos','Elo hasta 2006-12-08.xlsx'),\n",
    "#                             usecols = ['Date','Club','Elo'])\n",
    "\n",
    "# df_elo_test\n",
    "\n",
    "# def ObtenerRankingELO(fecha, country = 'ENG'):\n",
    "#     df_elo = pd.read_csv('http://api.clubelo.com/%s' % fecha, usecols = ['Club','Country','Elo'])\n",
    "#     df_elo = df_elo[df_elo['Country'] == country].reset_index(drop=True)\n",
    "#     df_elo['Date'] = fecha\n",
    "#     return df_elo[['Date','Club', 'Elo']]\n",
    "\n",
    "# df_cal = pd.read_excel(os.path.join(os.path.pardir,'Datos','Simulacion','Calibracion','Inglaterra.xlsx'))\n",
    "\n",
    "# %%time\n",
    "# fechas = df_cal['Date'].drop_duplicates().tolist()\n",
    "# fechas = [f for f in fechas if f not in df_elo_test['Date'].drop_duplicates().tolist()]\n",
    "# dfs_elo = []\n",
    "# errores = []\n",
    "# for fecha in fechas:\n",
    "#     print('Fecha actual: %s' % fecha, end ='\\t\\r')\n",
    "#     try:\n",
    "#         dfs_elo.append(ObtenerRankingELO(fecha, country = 'ENG'))\n",
    "#     except:\n",
    "#         errores.append(fecha)\n",
    "\n",
    "# df_elo = pd.read_excel(os.path.join(os.path.pardir, 'Datos', 'ELO Inglaterra.xlsx'), usecols = ['Date','Club','Elo'])\n",
    "\n",
    "# prereplaces = {'Man ': 'Manchester ',\n",
    "#                'QPR': 'Queens Park Rangers',\n",
    "#                'Middlesboro': 'Middlesbrough',\n",
    "#                'Wolves': 'Wolverhampton Wanderers',\n",
    "#                'Sheffield Weds': 'Sheffield Wednesday'}\n",
    "# df_elo = df_elo.replace(prereplaces, regex = True)\n",
    "\n",
    "# df_elo.head()\n",
    "\n",
    "# equipos_elo = df_elo['Club'].drop_duplicates().tolist()\n",
    "# equipos_cal = df_cal['Local'].drop_duplicates().tolist()\n",
    "# dictreplaces = {}\n",
    "# for eq in equipos_elo:\n",
    "#     eq_cal = [i for i in equipos_cal if eq in i]\n",
    "#     if eq_cal:\n",
    "#         dictreplaces[eq] = eq_cal[0]\n",
    "\n",
    "# df_elo = df_elo.replace(dictreplaces, regex = True)\n",
    "\n",
    "# df_elo_l = df_elo.copy()\n",
    "# df_elo_l.columns = ['Date','Local','elo_local']\n",
    "# df_elo_v = df_elo.copy()\n",
    "# df_elo_v.columns = ['Date','Visita','elo_visita']\n",
    "\n",
    "# df_cal = df_cal.merge(df_elo_l, how = 'left', left_on = ['Date','Local'], right_on = ['Date','Local'])\n",
    "\n",
    "# df_cal = df_cal.merge(df_elo_v, how = 'left', left_on = ['Date','Visita'], right_on = ['Date','Visita'])\n",
    "\n",
    "# df_cal_check = df_cal[['Local','Visita','elo_local','elo_visita']]\n",
    "\n",
    "# df_cal_check[df_cal_check.isnull().any(axis=1)]\n",
    "\n",
    "# df_cal.to_excel(os.path.join(os.path.pardir,'Datos','Simulacion','Calibracion','Inglaterra.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml_tags(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_links_tabla(tabla,html_root, tipo_tabla):\n",
    "    table_rows = tabla.find_all('tr')\n",
    "    urls = []\n",
    "    banned = ['off','final','cup','abstieg', 'sued', 'nord', 'endrunde']\n",
    "    for tr in table_rows:\n",
    "        tds = tr.find_all('td')\n",
    "        for td in tds:\n",
    "            if  not any(x in str(td) for x in banned) and tipo_tabla in str(td):\n",
    "                urls.append(td.find('a').get('href'))\n",
    "    if tipo_tabla == 'competition':\n",
    "        urls = [html_root + l.replace('competition','history') for l in urls]\n",
    "    if tipo_tabla == 'schedule':\n",
    "        urls = [html_root + l.replace('schedule','all_matches') for l in urls]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener datos de estadios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_estadios(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    html_root = url.split('/venues')[0]\n",
    "    soup = BeautifulSoup(html, features = \"xml\")\n",
    "    col_names = ['Stadium', 'City', 'Country', 'Capacity','Venue']\n",
    "    if 'Stadiums to be announced' not in str(soup):\n",
    "        # Titulo de la liga\n",
    "        liga = soup.findAll(\"title\")\n",
    "        liga = cleanhtml_tags(str(liga[0])).replace(' » Stadiums','').strip()\n",
    "        options = soup.findAll(\"select\")\n",
    "        if 'Stadiums' in str(options):\n",
    "            tables = soup.findAll(\"table\") \n",
    "            table_rows = tables[0].find_all('tr')\n",
    "            data = []\n",
    "            for tr in table_rows:\n",
    "                td = tr.find_all('td')\n",
    "                ref = tr.find_all('a')\n",
    "                row = [tr.text.strip() for tr in td if tr.text.strip()]\n",
    "                if row:\n",
    "                    row = [i.strip() for i in row if type(i) == str]\n",
    "                    row = [i.replace(\"???\",\"-\") for i in row]\n",
    "                    if ref:\n",
    "                        row.append(html_root + str(ref[0].get('href')))\n",
    "                    data.append(row)\n",
    "            df_estadios = pd.DataFrame(data = data, columns = col_names)\n",
    "            df_estadios['Torneo'] = liga.replace('Stadiums','').strip()\n",
    "            df_estadios['Capacity'] = df_estadios['Capacity'].str.replace('.','')\n",
    "            df_estadios['Capacity'] = df_estadios['Capacity'].replace('-',0, regex = False)\n",
    "            df_estadios['Capacity'] = df_estadios['Capacity'].astype(int)           \n",
    "    else:\n",
    "        df_estadios = pd.DataFrame(columns = col_names)\n",
    "    return df_estadios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_ligas(url):\n",
    "    html_root = url.split('/continents')[0]\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, features = \"xml\")\n",
    "    tables = soup.findAll(\"table\") \n",
    "    urls = obtener_links_tabla(tables[1],html_root, tipo_tabla = 'competition')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_torneos(url):\n",
    "    html_root = url.split('/history')[0]\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, features = \"xml\")\n",
    "    tables = soup.findAll(\"table\") \n",
    "    urls = obtener_links_tabla(tables[0],html_root, tipo_tabla = 'competition')\n",
    "    urls = [u.split('-spieltag')[0] for u in urls]\n",
    "    urls = [u.replace('history','all_matches') for u in urls]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_metadatos(report_url):\n",
    "    \"\"\"\n",
    "    Recibe una url del tipo:\n",
    "    https://www.worldfootball.net/report/premier-league-2018-2019-manchester-united-leicester-city/\n",
    "    y retorna la url de entrada, la url del estadio y la asistencia a un partido.\n",
    "    \"\"\"\n",
    "    Venue, Attendance = \"\", \"\"\n",
    "    try:        \n",
    "        html = urllib.request.urlopen(report_url).read()\n",
    "        html_root = report_url.split('/report')[0]\n",
    "        strainer = SoupStrainer('table')\n",
    "        soup = BeautifulSoup(html, 'lxml', parse_only=strainer)\n",
    "        table = [t for t in soup.findAll(\"table\") if 'venue' in str(t) and 'Attendance' in str(t)]\n",
    "        table = table[0]\n",
    "        trs = table.find_all('tr')\n",
    "        ref = trs[0].find_all('a')\n",
    "        if ref:\n",
    "            Venue = html_root + str(ref[0].get('href'))\n",
    "        tds = trs[1].find_all('td')\n",
    "        Attendance = ''.join(c for c in tds[2].text if not c.isspace() and '.' not in c)\n",
    "        if 'spectators' in Attendance:\n",
    "            Attendance = 0\n",
    "        try:\n",
    "            Attendance = int(Attendance)\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    return [report_url, Venue, Attendance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener partidos de un torneo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_tabla_temporada(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    html_root = url.split('/all_matches')[0]\n",
    "    soup = BeautifulSoup(html, features = \"xml\")\n",
    "    tables = soup.findAll(\"table\")\n",
    "    liga = soup.findAll(\"title\")\n",
    "    info = cleanhtml_tags(str(liga[0])).split(' » ')\n",
    "    Torneo = info[0]\n",
    "    table_rows = [t for t in tables if 'teams' in str(t) and 'schedule' in str(t)][0].find_all('tr')\n",
    "    return table_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_partidos(url, metadata = True):\n",
    "    \"\"\"\n",
    "    Función que recibe una url de worldfootball del tipo:\n",
    "    https://www.worldfootball.net/all_matches/chi-primera-division-2018/\n",
    "    Retorna un DataFrame que contiene las columnas:\n",
    "    Fecha, Torneo, Round, Local, Visita, goles L, goles V, Report\n",
    "    \"\"\"\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    print('Url actual: %s                                                                    ' % url, end = '\\t\\r')\n",
    "    html_root = url.split('/all_matches')[0]\n",
    "    soup = BeautifulSoup(html, features = \"xml\")\n",
    "    tables = soup.findAll(\"table\")\n",
    "    liga = soup.findAll(\"title\")\n",
    "    info = cleanhtml_tags(str(liga[0])).split(' » ')\n",
    "    Torneo = info[0].replace('Schedule','').strip()\n",
    "    table_rows = [t for t in tables if 'teams' in str(t) and 'schedule' in str(t)][0].find_all('tr')\n",
    "    matches = []\n",
    "    columns = ['Date','Torneo', 'Round', 'Local','Visita','goles L','goles V','Report']\n",
    "    Fecha = ''\n",
    "    Round = 0\n",
    "    for tr in table_rows:\n",
    "        tds = tr.find_all(['th','td'])\n",
    "        if len(tds) < 2 and '<th' in str(tds[0]):\n",
    "            Round = int(tds[0].find('a').text.replace('. Round',''))\n",
    "        elif len(tds) > 2 and 'news' not in str(tds[0]):\n",
    "            if 'matches' in str(tds[0]):\n",
    "                dmy = ''.join(c for c in cleanhtml_tags(str(tds[0])) if not c.isspace() and '.' not in c).split('/')\n",
    "                if len(dmy) == 3:\n",
    "                    Fecha = dmy[2][:4] + '-' + dmy[1] + '-' + dmy[0]\n",
    "            Local = tds[2].find('a').get('title')\n",
    "            Visita = tds[4].find('a').get('title')\n",
    "            outcomes = tds[5].text\n",
    "            if '(' in outcomes:\n",
    "                outcomes = outcomes.replace('(','').replace(')','').split(' ')\n",
    "                final_outcome = outcomes[0].split(':')\n",
    "            else:\n",
    "                final_outcome = outcomes.split(':')\n",
    "            if len(final_outcome) > 1:\n",
    "                golesL = re.sub(\"[^0-9]\", \"\", final_outcome[0])\n",
    "                golesV = re.sub(\"[^0-9]\", \"\", final_outcome[1])\n",
    "            else:\n",
    "                golesL = \"\"\n",
    "                golesV = \"\"\n",
    "            if 'report' in str(tds[5]):\n",
    "                report_url = html_root + tds[5].find('a').get('href')\n",
    "            else:\n",
    "                report_url = \"\"\n",
    "            if metadata:\n",
    "                if golesL != \"\" and golesV != \"\":\n",
    "                    matches.append([Fecha, Torneo, Round, Local, Visita, golesL, golesV, report_url])\n",
    "            else:\n",
    "                matches.append([Fecha, Torneo, Round, Local, Visita, golesL, golesV, report_url])\n",
    "    df_matches = pd.DataFrame(data = matches, columns = columns)\n",
    "    df_matches = df_matches.drop_duplicates()\n",
    "    if metadata:\n",
    "        df_matches['goles L'] = df_matches['goles L'].astype(int)\n",
    "        df_matches['goles V'] = df_matches['goles V'].astype(int)\n",
    "    else:\n",
    "        df_matches = df_matches[['Date','Torneo','Round','Local','Visita']]\n",
    "    return df_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_data(leagues, outputdir):\n",
    "    for league in leagues:\n",
    "        filename = league + '.xlsx'\n",
    "        url_liga = 'https://www.worldfootball.net/history/' + league + '/'\n",
    "        print('--------------------------------------------------------------------------------------------------------')\n",
    "        print('Generando: %s' % filename)\n",
    "        ti = time()\n",
    "        urls_torneos = obtener_torneos(url_liga)\n",
    "        print('Número de torneos: %s' % len(urls_torneos))\n",
    "        urls_estadios = [u.replace('all_matches','venues') for u in urls_torneos]\n",
    "        dfs_matches = Parallel(n_jobs = 16)(delayed(obtener_partidos)(url) for url in urls_torneos)\n",
    "        dfs_estadios = Parallel(n_jobs = 16)(delayed(info_estadios)(url) for url in urls_estadios)\n",
    "        df_matches = pd.concat(dfs_matches, ignore_index = True)\n",
    "        df_estadios = pd.concat(dfs_estadios, ignore_index = True, sort = True)\n",
    "        urls_reports = df_matches['Report'].tolist()\n",
    "        urls_reports = [u for u in urls_reports if u != \"\"]\n",
    "        print('Número de reportes: %s' % len(urls_reports))\n",
    "        metadata = Parallel(n_jobs = 60)(delayed(obtener_metadatos)(url) for url in urls_reports)\n",
    "        print('Metadata obtenida')\n",
    "        df_metadata = pd.DataFrame(data = metadata, columns = ['Report','Venue','Attendance'])\n",
    "        df_matches = pd.merge(df_matches, df_metadata, how = 'left', on = ['Report'])\n",
    "        df_matches = pd.merge(df_matches, df_estadios, how = 'left', on = ['Torneo','Venue'])\n",
    "        df_matches = df_matches[[i for i in df_matches.columns if i not in ['Report','Venue']] + ['Report','Venue']]\n",
    "        print('Guardando archivo')\n",
    "        df_matches.to_excel(os.path.join(outputdir,filename), index = False)\n",
    "        tf = time()\n",
    "        deltat = str(datetime.timedelta(seconds=tf - ti))[:7]\n",
    "        print('Ejecución terminada en %s' % deltat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = os.path.join(os.path.pardir,'Datos','Ligas')\n",
    "leagues = ['arg-primera-division']\n",
    "exportar_data(leagues, outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_fixture = obtener_partidos('https://www.worldfootball.net/all_matches/eng-premier-league-2018-2019/', metadata = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir = os.path.join(os.path.pardir, 'Datos','Fixtures')\n",
    "# df_fixture.to_excel(os.path.join(datadir, 'eng-premier-league.xlsx'), sheet_name = 'Original', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
